<div align="center">
  <img src="assets/logo.png" alt="Tachikoma Logo" width="200">
  <h1>Tachikoma</h1>
  <p><em>A comprehensive Swift package for AI model integration</em></p>
  
  [![Swift 6](https://img.shields.io/badge/Swift-6.0+-orange.svg)](https://swift.org)
  [![Platforms](https://img.shields.io/badge/platforms-macOS%20%7C%20iOS%20%7C%20watchOS%20%7C%20tvOS-lightgrey.svg)](https://github.com/steipete/Tachikoma)
  [![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
  [![Tests](https://img.shields.io/badge/tests-passing-green.svg)](#testing)
</div>

Named after the spider-tank AI from Ghost in the Shell, **Tachikoma** provides an intelligent, adaptable interface for AI services. This comprehensive Swift package offers a unified API for multiple AI providers including OpenAI, Anthropic, Grok (xAI), and Ollama, built with Swift 6 strict concurrency mode for maximum safety and performance.

## ğŸš€ Why Tachikoma?

```ascii
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Traditional AI Integration                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚   OpenAI    â”‚  â”‚  Anthropic  â”‚  â”‚    Grok     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚     SDK     â”‚  â”‚     SDK     â”‚  â”‚     SDK     â”‚  â”‚   ...   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                 â”‚                 â”‚            â”‚     â”‚
â”‚         â–¼                 â–¼                 â–¼            â–¼     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚           Your Application Code                             â”‚ â”‚
â”‚  â”‚      â€¢ Different APIs for each provider                    â”‚ â”‚
â”‚  â”‚      â€¢ Inconsistent error handling                         â”‚ â”‚
â”‚  â”‚      â€¢ Manual streaming implementation                     â”‚ â”‚
â”‚  â”‚      â€¢ Provider-specific message formats                   â”‚ â”‚
â”‚  â”‚      â€¢ No unified tool calling                             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Tachikoma Architecture                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                Your Application Code                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                        â”‚                                         â”‚
â”‚                        â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                Tachikoma Unified API                        â”‚ â”‚
â”‚  â”‚    â€¢ Single ModelInterface for all providers               â”‚ â”‚
â”‚  â”‚    â€¢ Unified message types & streaming                     â”‚ â”‚
â”‚  â”‚    â€¢ Consistent error handling                             â”‚ â”‚
â”‚  â”‚    â€¢ Type-safe tool calling                                â”‚ â”‚
â”‚  â”‚    â€¢ Swift 6 concurrency safety                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                â”‚   â”‚           â”‚           â”‚                   â”‚
â”‚                â–¼   â–¼           â–¼           â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   OpenAI    â”‚  â”‚  Anthropic  â”‚  â”‚    Grok     â”‚  â”‚ Ollama  â”‚ â”‚
â”‚  â”‚  Provider   â”‚  â”‚  Provider   â”‚  â”‚  Provider   â”‚  â”‚Provider â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âœ¨ Key Advantages

- **ğŸ”— Unified Interface**: One API for all providers - switch between OpenAI, Claude, Grok, and Ollama with a single line change
- **ğŸ›¡ï¸ Swift 6 Safety**: Built with strict concurrency mode, eliminating data races and ensuring thread safety
- **âš¡ High Performance**: Intelligent caching, streaming optimization, and memory-efficient processing
- **ğŸ¯ Type Safety**: Strongly-typed message system prevents runtime errors and improves code reliability
- **ğŸ”§ Tool Calling**: Comprehensive function calling support with generic context management
- **ğŸ“± Multimodal**: Native support for text, images, audio, and file inputs across all compatible providers
- **ğŸš€ Async/Await**: Modern Swift concurrency patterns throughout

## ğŸ“‹ Table of Contents

- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Core Features](#-core-features)
- [Provider Comparison](#-provider-comparison)
- [Advanced Usage](#-advanced-usage)
- [Architecture Overview](#-architecture-overview)
- [Performance & Benchmarks](#-performance--benchmarks)
- [Testing](#-testing)
- [Migration Guide](#-migration-guide)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)

## ğŸ“¦ Installation

### Swift Package Manager

Add Tachikoma as a dependency in your `Package.swift`:

```swift
dependencies: [
    .package(url: "https://github.com/steipete/Tachikoma", from: "1.0.0")
],
targets: [
    .target(
        name: "YourTarget",
        dependencies: ["Tachikoma"]
    )
]
```

### Xcode Project

1. File â†’ Add Package Dependencies
2. Enter: `https://github.com/steipete/Tachikoma`
3. Select version: `1.0.0` or later

### System Requirements

| Platform | Minimum Version | Swift Version |
|----------|-----------------|---------------|
| macOS    | 14.0+          | 6.0+          |
| iOS      | 17.0+          | 6.0+          |
| watchOS  | 10.0+          | 6.0+          |
| tvOS     | 17.0+          | 6.0+          |
| Xcode    | 16.0+          | -             |

## ğŸš€ Quick Start

### 1. Basic Setup

```swift
import Tachikoma

// Get a model instance - Tachikoma handles provider selection
let tachikoma = Tachikoma.shared
let model = try await tachikoma.getModel("claude-opus-4")

// Create a simple request
let request = ModelRequest(
    messages: [
        .user(content: .text("What is the capital of France?"))
    ]
)

// Get response
let response = try await model.getResponse(request: request)
print(response.content.first?.text ?? "No response")
// Output: "The capital of France is Paris."
```

### 2. Streaming Responses

```swift
let request = ModelRequest(
    messages: [.user(content: .text("Write a haiku about artificial intelligence"))],
    settings: ModelSettings(temperature: 0.7)
)

print("ğŸ¤– AI Response:")
for try await event in try await model.getStreamedResponse(request: request) {
    switch event {
    case .textDelta(let delta):
        print(delta.delta, terminator: "")
    case .responseCompleted(let completion):
        print("\nâœ… Completed | Tokens: \(completion.usage?.totalTokens ?? 0)")
    case .error(let error):
        print("\nâŒ Error: \(error.message)")
    default:
        break
    }
}
```

### 3. Provider Switching

```swift
// Switch providers with zero code changes
let claudeModel = try await tachikoma.getModel("claude-opus-4")
let gptModel = try await tachikoma.getModel("gpt-4.1") 
let grokModel = try await tachikoma.getModel("grok-4")
let ollamaModel = try await tachikoma.getModel("llama3.3")

// Same request works with all providers
let request = ModelRequest(messages: [.user(content: .text("Hello!"))])

let claudeResponse = try await claudeModel.getResponse(request: request)
let gptResponse = try await gptModel.getResponse(request: request)
let grokResponse = try await grokModel.getResponse(request: request)
let ollamaResponse = try await ollamaModel.getResponse(request: request)
```

## ğŸ”§ Core Features

### Message Type System

Tachikoma's type-safe message system prevents runtime errors and provides compile-time guarantees:

```ascii
MessageContent
â”œâ”€â”€ text(String)
â”œâ”€â”€ imageUrl(ImageUrl)
â”œâ”€â”€ imageBase64(ImageBase64)
â”œâ”€â”€ audioContent(AudioContent)
â”œâ”€â”€ fileContent(FileContent)
â””â”€â”€ multimodal([MessageContentPart])
    â”œâ”€â”€ text(String)
    â”œâ”€â”€ imageUrl(ImageUrl)
    â”œâ”€â”€ imageBase64(ImageBase64)
    â”œâ”€â”€ audioContent(AudioContent)
    â””â”€â”€ fileContent(FileContent)
```

### Comprehensive Multimodal Support

#### Text + Image Analysis

```swift
let imageData = try Data(contentsOf: URL(string: "https://example.com/chart.png")!)
let base64Image = imageData.base64EncodedString()

let request = ModelRequest(
    messages: [
        .user(content: .multimodal([
            .text("Analyze this sales chart and provide key insights:"),
            .imageBase64(ImageBase64(
                base64: base64Image,
                mediaType: "image/png",
                detail: .high  // Use high detail for charts/documents
            ))
        ]))
    ],
    settings: ModelSettings(temperature: 0.1) // Lower temperature for analysis
)

let response = try await model.getResponse(request: request)
```

#### Audio Transcription + Analysis

```swift
let audioData = try Data(contentsOf: audioFileURL)

let request = ModelRequest(
    messages: [
        .user(content: .multimodal([
            .text("Transcribe and summarize the key points from this meeting:"),
            .audioContent(AudioContent(
                data: audioData,
                format: .mp3,
                transcript: nil // Will be transcribed if supported
            ))
        ]))
    ]
)
```

#### File Processing

```swift
let pdfData = try Data(contentsOf: documentURL)

let request = ModelRequest(
    messages: [
        .user(content: .multimodal([
            .text("Extract the main conclusions from this research paper:"),
            .fileContent(FileContent(
                data: pdfData,
                filename: "research_paper.pdf",
                mediaType: "application/pdf"
            ))
        ]))
    ]
)
```

### Advanced Tool Calling

#### Simple Function Definition

```swift
// Define a weather tool
let weatherTool = ToolDefinition(
    function: FunctionDefinition(
        name: "get_weather",
        description: "Get current weather information for a specific location",
        parameters: ToolParameters(
            type: "object",
            properties: [
                "location": ParameterSchema(
                    type: .string,
                    description: "The city and country, e.g. 'San Francisco, CA'"
                ),
                "units": ParameterSchema(
                    type: .string,
                    description: "Temperature units",
                    enumValues: ["celsius", "fahrenheit"]
                )
            ],
            required: ["location"]
        )
    )
)

let request = ModelRequest(
    messages: [.user(content: .text("What's the weather in Tokyo?"))],
    tools: [weatherTool],
    settings: ModelSettings(
        toolChoice: .auto,  // Let model decide when to use tools
        temperature: 0.3
    )
)

let response = try await model.getResponse(request: request)

// Handle tool calls
for content in response.content {
    switch content {
    case .toolCall(let toolCall):
        print("ğŸ”§ Tool Called: \(toolCall.function.name)")
        print("ğŸ“ Arguments: \(toolCall.function.arguments)")
        
        // Execute your tool logic here
        let weatherData = await getWeatherData(from: toolCall.function.arguments)
        
        // Send results back to the model
        let followUpRequest = ModelRequest(
            messages: [
                .tool(toolCallId: toolCall.id, content: weatherData)
            ]
        )
        
    case .outputText(let text):
        print("ğŸ’¬ Response: \(text)")
    }
}
```

#### Generic Tool Context System

```swift
// Define a typed context for database operations
struct DatabaseContext {
    let connection: DatabaseConnection
    let schema: DatabaseSchema
    let userPermissions: UserPermissions
}

// Create a type-safe tool with context
let dbTool = Tool<DatabaseContext> { input, context in
    guard context.userPermissions.canRead else {
        throw ToolError.unauthorized("Read access denied")
    }
    
    let query = input.parameters["query"] as? String ?? ""
    let results = try await context.connection.execute(query)
    
    return ToolOutput(
        content: results.formatted(),
        metadata: ["rowCount": results.count]
    )
}

// Use with context
let dbContext = DatabaseContext(
    connection: myConnection,
    schema: mySchema, 
    userPermissions: currentUser.permissions
)

let toolDefinition = dbTool.toToolDefinition()
```

### Streaming Event Processing

```swift
func handleStreamingResponse(_ stream: AsyncThrowingStream<StreamEvent, Error>) async {
    var currentText = ""
    var toolCalls: [String: PartialToolCall] = [:]
    
    for try await event in stream {
        switch event {
        case .responseStarted(let start):
            print("ğŸš€ Response started (ID: \(start.id), Model: \(start.model))")
            
        case .textDelta(let delta):
            currentText += delta.delta
            print(delta.delta, terminator: "")
            
        case .toolCallDelta(let delta):
            // Accumulate tool call data
            if toolCalls[delta.id] == nil {
                toolCalls[delta.id] = PartialToolCall(id: delta.id)
            }
            toolCalls[delta.id]?.append(delta)
            
        case .toolCallCompleted(let completed):
            print("\nğŸ”§ Tool call completed: \(completed.function.name)")
            // Execute tool and send results back
            
        case .responseCompleted(let completion):
            print("\nâœ… Stream completed")
            print("ğŸ“Š Usage: \(completion.usage?.totalTokens ?? 0) tokens")
            
        case .error(let error):
            print("\nâŒ Stream error: \(error.message)")
            if error.isRetryable {
                print("ğŸ”„ Retrying in \(error.retryAfter ?? 1) seconds...")
            }
            
        case .reasoningSummaryDelta(let reasoning):
            print("\nğŸ§  Reasoning: \(reasoning.delta)")
            
        default:
            break
        }
    }
}
```

## ğŸ”€ Provider Comparison

| Feature | OpenAI | Anthropic | Grok | Ollama |
|---------|--------|-----------|------|--------|
| **Models** | GPT-4o, GPT-4.1, o3, o4 | Claude 4, Claude 3.5/3.7 | Grok 4, Grok 3, Grok 2 | Llama 3.3, Mistral, etc. |
| **Tool Calling** | âœ… Full Support | âœ… Full Support | âœ… Full Support | âš ï¸ Select Models |
| **Streaming** | âœ… SSE + Chunked | âœ… SSE | âœ… SSE | âœ… HTTP Streaming |
| **Vision** | âœ… GPT-4o/4.1 | âœ… Claude 3+ | âœ… Grok 2 Vision | âœ… LLaVA Models |
| **Audio** | âœ… Whisper Integration | âŒ External Required | âŒ External Required | âŒ External Required |
| **Context Length** | 128K-1M tokens | 200K tokens | 32K-128K tokens | Varies by model |
| **Reasoning** | âœ… o3/o4 Models | âœ… Thinking Modes | âš ï¸ Basic | âš ï¸ Model Dependent |
| **Local Deployment** | âŒ Cloud Only | âŒ Cloud Only | âŒ Cloud Only | âœ… Full Local |
| **Cost** | $0.50-$60/1M tokens | $0.25-$75/1M tokens | $0.50-$5/1M tokens | Free (Local) |

### Provider-Specific Features

#### OpenAI Unique Capabilities
```swift
// O3/O4 Reasoning Models
let reasoningRequest = ModelRequest(
    messages: [.user(content: .text("Solve this complex math problem step by step"))],
    settings: ModelSettings(
        modelName: "o3",
        additionalParameters: ModelParameters()
            .with("reasoning_effort", value: "high")  // high, medium, low
            .with("max_completion_tokens", value: 32768)
    )
)

// Dual API Support - automatically handled
let chatModel = try await tachikoma.getModel("gpt-4.1")      // Uses Chat Completions API
let reasoningModel = try await tachikoma.getModel("o3")      // Uses Responses API
```

#### Anthropic Claude Features
```swift
// Extended Thinking Mode
let thinkingRequest = ModelRequest(
    messages: [.user(content: .text("Design a complex software architecture"))],
    settings: ModelSettings(
        modelName: "claude-opus-4-thinking",  // Enables thinking mode
        temperature: 0.7
    )
)

// System Prompt Caching (for repeated similar requests)
let systemPrompt = "You are an expert software architect with 20 years of experience..."
let cachedRequest = ModelRequest(
    messages: [
        .system(content: systemPrompt),  // Will be cached automatically
        .user(content: .text("Design a microservices architecture"))
    ]
)
```

#### Grok Performance Optimizations
```swift
// Optimized for speed
let grokRequest = ModelRequest(
    messages: [.user(content: .text("Quick factual question"))],
    settings: ModelSettings(
        modelName: "grok-4",
        temperature: 0.1,  // Grok performs well with low temperature
        maxTokens: 1000    // Grok 4 has parameter filtering
    )
)
```

#### Ollama Local Deployment
```swift
// Configure custom Ollama endpoint
let config = ProviderConfiguration.ollama(
    baseURL: URL(string: "http://gpu-server:11434")!,
    timeout: 300  // 5 minutes for model loading
)
await Tachikoma.shared.configureProvider(config)

// Use local models
let localModel = try await tachikoma.getModel("llama3.3")
let visionModel = try await tachikoma.getModel("llava:latest")

// Tool calling only works with compatible models
let toolCompatibleModels = [
    "llama3.3", "llama3.2", "mistral-nemo", 
    "firefunction-v2", "command-r-plus"
]
```

## ğŸ“ˆ Advanced Usage

### Concurrent Request Processing

```swift
// Process multiple requests concurrently
let requests = [
    ("Summarize this article", article1),
    ("Translate to Spanish", text2),
    ("Analyze sentiment", review3),
    ("Extract entities", document4)
]

let responses = try await withThrowingTaskGroup(of: (String, ModelResponse).self) { group in
    for (task, content) in requests {
        group.addTask {
            let model = try await tachikoma.getModel("claude-sonnet-4")
            let request = ModelRequest(
                messages: [.user(content: .text("\(task): \(content)"))]
            )
            let response = try await model.getResponse(request: request)
            return (task, response)
        }
    }
    
    var results: [(String, ModelResponse)] = []
    for try await result in group {
        results.append(result)
    }
    return results
}

for (task, response) in responses {
    print("\(task): \(response.content.first?.text ?? "No response")")
}
```

### Custom Provider Implementation

```swift
// Implement ModelInterface for custom AI providers
class CustomAIProvider: ModelInterface {
    let apiKey: String
    let baseURL: URL
    
    var maskedApiKey: String {
        guard apiKey.count > 8 else { return "***" }
        return "\(apiKey.prefix(6))...\(apiKey.suffix(2))"
    }
    
    init(apiKey: String, baseURL: URL) {
        self.apiKey = apiKey
        self.baseURL = baseURL
    }
    
    func getResponse(request: ModelRequest) async throws -> ModelResponse {
        // Custom implementation for your AI service
        let httpRequest = try buildHTTPRequest(from: request)
        let (data, _) = try await URLSession.shared.data(for: httpRequest)
        return try parseResponse(data)
    }
    
    func getStreamedResponse(request: ModelRequest) async throws -> AsyncThrowingStream<StreamEvent, Error> {
        // Custom streaming implementation
        return AsyncThrowingStream { continuation in
            Task {
                do {
                    let httpRequest = try buildStreamingRequest(from: request)
                    let (bytes, _) = try await URLSession.shared.bytes(for: httpRequest)
                    
                    for try await line in bytes.lines {
                        let event = try parseStreamEvent(line)
                        continuation.yield(event)
                    }
                    continuation.finish()
                } catch {
                    continuation.finish(throwing: error)
                }
            }
        }
    }
    
    private func buildHTTPRequest(from request: ModelRequest) throws -> URLRequest {
        // Convert ModelRequest to your API format
        var httpRequest = URLRequest(url: baseURL.appendingPathComponent("chat"))
        httpRequest.httpMethod = "POST"
        httpRequest.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
        httpRequest.setValue("application/json", forHTTPHeaderField: "Content-Type")
        
        let requestBody = CustomAPIRequest(
            messages: request.messages.map(convertMessage),
            temperature: request.settings?.temperature,
            maxTokens: request.settings?.maxTokens
        )
        
        httpRequest.httpBody = try JSONEncoder().encode(requestBody)
        return httpRequest
    }
}

// Register your custom provider
await Tachikoma.shared.registerModel(name: "custom-ai") {
    CustomAIProvider(
        apiKey: ProcessInfo.processInfo.environment["CUSTOM_AI_KEY"] ?? "",
        baseURL: URL(string: "https://api.custom-ai.com/v1")!
    )
}

// Use like any other provider
let customModel = try await tachikoma.getModel("custom-ai")
```

### Advanced Error Handling

```swift
func robustAIInteraction() async {
    let maxRetries = 3
    var retryCount = 0
    
    while retryCount < maxRetries {
        do {
            let model = try await tachikoma.getModel("claude-opus-4")
            let request = ModelRequest(
                messages: [.user(content: .text("Important query that must succeed"))]
            )
            
            let response = try await model.getResponse(request: request)
            print("âœ… Success: \(response.content.first?.text ?? "")")
            return
            
        } catch let error as TachikomaError {
            switch error {
            case .rateLimited:
                let delay = error.retryAfter ?? Double(2 << retryCount) // Exponential backoff
                print("â±ï¸ Rate limited, waiting \(delay) seconds...")
                try await Task.sleep(for: .seconds(delay))
                
            case .contextLengthExceeded:
                print("ğŸ“ Context too long, trying with shorter input...")
                // Implement context truncation logic
                
            case .insufficientQuota:
                print("ğŸ’³ Quota exceeded, switching to backup provider...")
                // Switch to alternative provider
                
            case .networkError(let underlying):
                print("ğŸŒ Network error: \(underlying.localizedDescription)")
                if error.isRetryable {
                    try await Task.sleep(for: .seconds(1))
                } else {
                    throw error
                }
                
            default:
                print("âŒ Unrecoverable error: \(error.localizedDescription)")
                if let suggestion = error.recoverySuggestion {
                    print("ğŸ’¡ Suggestion: \(suggestion)")
                }
                throw error
            }
        }
        
        retryCount += 1
    }
    
    throw TachikomaError.maxRetriesExceeded("Failed after \(maxRetries) attempts")
}
```

### Configuration Management

```swift
// Environment-based configuration
class AIConfiguration {
    static func setupFromEnvironment() async throws {
        let tachikoma = Tachikoma.shared
        
        // OpenAI configuration
        if let openAIKey = ProcessInfo.processInfo.environment["OPENAI_API_KEY"],
           !openAIKey.isEmpty {
            let config = ProviderConfiguration.openAI(
                apiKey: openAIKey,
                baseURL: URL(string: ProcessInfo.processInfo.environment["OPENAI_BASE_URL"] 
                            ?? "https://api.openai.com/v1")!,
                organizationId: ProcessInfo.processInfo.environment["OPENAI_ORG_ID"]
            )
            try await tachikoma.configureProvider(config)
        }
        
        // Anthropic configuration
        if let anthropicKey = ProcessInfo.processInfo.environment["ANTHROPIC_API_KEY"],
           !anthropicKey.isEmpty {
            let config = ProviderConfiguration.anthropic(
                apiKey: anthropicKey,
                baseURL: URL(string: ProcessInfo.processInfo.environment["ANTHROPIC_BASE_URL"] 
                            ?? "https://api.anthropic.com")!
            )
            try await tachikoma.configureProvider(config)
        }
        
        // Grok configuration  
        if let grokKey = ProcessInfo.processInfo.environment["X_AI_API_KEY"] 
                      ?? ProcessInfo.processInfo.environment["XAI_API_KEY"],
           !grokKey.isEmpty {
            let config = ProviderConfiguration.grok(
                apiKey: grokKey,
                baseURL: URL(string: "https://api.x.ai/v1")!
            )
            try await tachikoma.configureProvider(config)
        }
        
        // Ollama configuration (local)
        let ollamaURL = ProcessInfo.processInfo.environment["PEEKABOO_OLLAMA_BASE_URL"] 
                       ?? "http://localhost:11434"
        let config = ProviderConfiguration.ollama(
            baseURL: URL(string: ollamaURL)!,
            timeout: 300
        )
        try await tachikoma.configureProvider(config)
        
        print("âœ… AI providers configured from environment")
    }
}

// Configuration file support
struct TachikomaConfig: Codable {
    struct Provider: Codable {
        let apiKey: String?
        let baseURL: String?
        let organizationId: String?
        let timeout: TimeInterval?
    }
    
    let openai: Provider?
    let anthropic: Provider?
    let grok: Provider?
    let ollama: Provider?
    let defaultModel: String?
}

func loadConfigFromFile() async throws {
    let configURL = FileManager.default.homeDirectoryForCurrentUser
        .appendingPathComponent(".tachikoma")
        .appendingPathComponent("config.json")
    
    let configData = try Data(contentsOf: configURL)
    let config = try JSONDecoder().decode(TachikomaConfig.self, from: configData)
    
    // Apply configuration
    // ... implementation details
}
```

## ğŸ—ï¸ Architecture Overview

### High-Level Component Diagram

```ascii
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                Application Layer                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   iOS App    â”‚  â”‚   macOS App  â”‚  â”‚  Server App  â”‚  â”‚  Command Line    â”‚   â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚  â”‚      Tool        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚                    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            â–¼                  â–¼         Tachikoma Package      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                        Tachikoma.shared                                 â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚   getModel()  â”‚ â”‚ configure() â”‚ â”‚ register()   â”‚ â”‚   clearCache()  â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    ModelProvider (Actor)                                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚   Registry   â”‚ â”‚    Cache     â”‚ â”‚   Factory    â”‚ â”‚  Name Resolver  â”‚ â”‚   â”‚
â”‚  â”‚  â”‚              â”‚ â”‚              â”‚ â”‚              â”‚ â”‚                 â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    ModelInterface                                       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚  getResponse(request: ModelRequest) async throws -> ModelResponse â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  getStreamedResponse(...) -> AsyncThrowingStream<StreamEvent, E>  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  maskedApiKey: String                                            â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚             â”‚             â”‚             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ OpenAIModel    â”‚ â”‚ AnthropicM â”‚ â”‚ GrokModel  â”‚ â”‚ OllamaModel â”‚
    â”‚                â”‚ â”‚    odel    â”‚ â”‚            â”‚ â”‚             â”‚
    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚ â”‚Chat Compl. â”‚ â”‚ â”‚ â”‚ Claude â”‚ â”‚ â”‚ â”‚OpenAI  â”‚ â”‚ â”‚ â”‚ Local   â”‚ â”‚
    â”‚ â”‚Responses   â”‚ â”‚ â”‚ â”‚Native  â”‚ â”‚ â”‚ â”‚Compat  â”‚ â”‚ â”‚ â”‚HTTP API â”‚ â”‚
    â”‚ â”‚API Supportâ”‚ â”‚ â”‚ â”‚SSE     â”‚ â”‚ â”‚ â”‚Format  â”‚ â”‚ â”‚ â”‚         â”‚ â”‚
    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Architecture

```ascii
Request Processing Flow:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. Request Creation
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Application   â”‚â”€â”€â”€â–¶â”‚   ModelRequest   â”‚â”€â”€â”€â–¶â”‚   Message Types     â”‚
   â”‚                 â”‚    â”‚                  â”‚    â”‚ â€¢ system/user/      â”‚
   â”‚ â€¢ User input    â”‚    â”‚ â€¢ messages[]     â”‚    â”‚   assistant/tool    â”‚
   â”‚ â€¢ Tool defs     â”‚    â”‚ â€¢ tools[]        â”‚    â”‚ â€¢ Content types     â”‚
   â”‚ â€¢ Settings      â”‚    â”‚ â€¢ settings       â”‚    â”‚ â€¢ Tool definitions  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. Provider Resolution
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   ModelProvider     â”‚â”€â”€â”€â–¶â”‚   Name Resolver  â”‚â”€â”€â”€â–¶â”‚   Model Factory  â”‚
   â”‚                     â”‚    â”‚                  â”‚    â”‚                  â”‚
   â”‚ â€¢ getModel("gpt-4") â”‚    â”‚ â€¢ "gpt" â†’ "gpt-4.1" â”‚ â”‚ â€¢ Provider check â”‚
   â”‚ â€¢ Cache lookup      â”‚    â”‚ â€¢ Lenient matching   â”‚ â”‚ â€¢ API key verify â”‚
   â”‚ â€¢ Factory creation  â”‚    â”‚ â€¢ Provider/model     â”‚ â”‚ â€¢ Instance createâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. Request Conversion
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   ModelInterface    â”‚â”€â”€â”€â–¶â”‚ Provider-Specific â”‚â”€â”€â”€â–¶â”‚   HTTP Request   â”‚
   â”‚                     â”‚    â”‚   Conversion      â”‚    â”‚                  â”‚
   â”‚ â€¢ Universal format  â”‚    â”‚ â€¢ OpenAI format   â”‚    â”‚ â€¢ Headers        â”‚
   â”‚ â€¢ Type validation   â”‚    â”‚ â€¢ Claude format   â”‚    â”‚ â€¢ JSON payload   â”‚
   â”‚ â€¢ Parameter mapping â”‚    â”‚ â€¢ Grok format     â”‚    â”‚ â€¢ Streaming opts â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. Network & Streaming
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   URLSession        â”‚â”€â”€â”€â–¶â”‚   Response Stream â”‚â”€â”€â”€â–¶â”‚   Event Parser   â”‚
   â”‚                     â”‚    â”‚                   â”‚    â”‚                  â”‚
   â”‚ â€¢ HTTP/2 support    â”‚    â”‚ â€¢ SSE processing  â”‚    â”‚ â€¢ Delta accum.   â”‚
   â”‚ â€¢ Connection pool   â”‚    â”‚ â€¢ Line buffering  â”‚    â”‚ â€¢ Tool call buildâ”‚
   â”‚ â€¢ Timeout handling  â”‚    â”‚ â€¢ Error detection â”‚    â”‚ â€¢ Event emission â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

5. Response Processing
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   StreamEvent       â”‚â”€â”€â”€â–¶â”‚   Event Handler   â”‚â”€â”€â”€â–¶â”‚   Application    â”‚
   â”‚                     â”‚    â”‚                   â”‚    â”‚                  â”‚
   â”‚ â€¢ textDelta         â”‚    â”‚ â€¢ Text accumul.   â”‚    â”‚ â€¢ UI updates     â”‚
   â”‚ â€¢ toolCallDelta     â”‚    â”‚ â€¢ Tool execution  â”‚    â”‚ â€¢ Tool responses â”‚
   â”‚ â€¢ responseCompleted â”‚    â”‚ â€¢ Error handling  â”‚    â”‚ â€¢ Final result   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Message Type Hierarchy

```ascii
Message Enumeration Structure:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                              Message
                                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚                       â”‚
   system(String)          user(content)         assistant(content[], status)
                                â”‚                       â”‚
                                â”‚                       â”‚
                         MessageContent          AssistantContent
                                â”‚                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚     â”‚     â”‚                 â”‚             â”‚
    text(String)    imageUrl  imageBase64  multimodal  outputText  toolCall  refusal
                       â”‚         â”‚          â”‚           â”‚           â”‚        â”‚
                       â”‚         â”‚          â”‚           â”‚           â”‚        â”‚
                   ImageUrl  ImageBase64    â”‚        String    ToolCallItem String
                      â”‚         â”‚          â”‚                      â”‚
                 â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â” â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”     â”‚                 â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                url     base64    mediaType â”‚                 id      function
               detail   detail             â”‚                type    FunctionCall
                                          â”‚                          â”‚
                                          â”‚                     â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                                          â”‚                   name    arguments
                              MessageContentPart[]
                                          â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚                 â”‚                 â”‚
                   text(String)     imageUrl           audioContent
                                   imageBase64         fileContent
                                                           â”‚
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”´â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚                     â”‚   â”‚                     â”‚
                             AudioContent         FileContent                ImageContent
                                  â”‚                     â”‚                         â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                         data          format  data        filename      base64     mediaType
                       transcript      duration mediaType  description   detail     width/height
```

## âš¡ Performance & Benchmarks

### Memory Usage Patterns

```swift
// Memory-efficient streaming (constant memory usage)
func measureStreamingMemory() async {
    let memoryBefore = getMemoryUsage()
    
    let model = try await tachikoma.getModel("claude-opus-4")
    let request = ModelRequest(
        messages: [.user(content: .text("Write a very long story (10,000 words)"))]
    )
    
    var tokenCount = 0
    for try await event in try await model.getStreamedResponse(request: request) {
        switch event {
        case .textDelta(let delta):
            tokenCount += estimateTokens(delta.delta)
            // Memory usage remains constant regardless of response length
        default:
            break
        }
    }
    
    let memoryAfter = getMemoryUsage()
    print("ğŸ“Š Processed \(tokenCount) tokens with \(memoryAfter - memoryBefore)MB memory increase")
    // Typical result: ~0.5-2MB increase regardless of response size
}
```

### Performance Benchmarks

| Operation | Cold Start | Warm Cache | Memory Peak | Concurrent Limit |
|-----------|------------|------------|-------------|-------------------|
| **Model Instantiation** | 5-15ms | <1ms | 2-5MB | Unlimited |
| **Simple Request** | 200-800ms | 150-600ms | 1-3MB | 50+ concurrent |
| **Streaming Start** | 250-900ms | 200-700ms | 1-3MB | 30+ concurrent |
| **Tool Call Processing** | +50-200ms | +30-150ms | +0.5-2MB | Provider dependent |
| **Large Context (50K tokens)** | 1-3s | 800ms-2s | 5-15MB | 10+ concurrent |

*Benchmarks measured on M1 MacBook Pro with 16GB RAM. Actual performance varies by provider and network conditions.*

### Optimization Strategies

```swift
// 1. Model Instance Caching
let modelCache = ModelCache.shared
let model = try await modelCache.getOrCreate("claude-opus-4") {
    // Factory only called once, then cached
    return try await tachikoma.getModel("claude-opus-4")
}

// 2. Request Batching for High Volume
class BatchProcessor {
    private var pendingRequests: [ModelRequest] = []
    private let batchSize = 10
    private let batchTimeout: TimeInterval = 0.1
    
    func addRequest(_ request: ModelRequest) async throws -> ModelResponse {
        return try await withCheckedThrowingContinuation { continuation in
            let batchItem = BatchItem(request: request, continuation: continuation)
            pendingRequests.append(batchItem)
            
            if pendingRequests.count >= batchSize {
                processBatch()
            }
        }
    }
    
    private func processBatch() {
        // Process multiple requests concurrently
        Task {
            await withTaskGroup(of: Void.self) { group in
                for item in pendingRequests {
                    group.addTask {
                        await processRequest(item)
                    }
                }
            }
            pendingRequests.removeAll()
        }
    }
}

// 3. Intelligent Connection Pooling
extension URLSession {
    static let optimizedSession: URLSession = {
        let config = URLSessionConfiguration.default
        config.httpMaximumConnectionsPerHost = 10
        config.timeoutIntervalForRequest = 60
        config.timeoutIntervalForResource = 300
        config.urlCache = URLCache(
            memoryCapacity: 50 * 1024 * 1024,    // 50MB memory
            diskCapacity: 100 * 1024 * 1024,     // 100MB disk
            diskPath: "tachikoma_cache"
        )
        return URLSession(configuration: config)
    }()
}
```

## ğŸ§ª Testing

### Running Tests

```bash
# Run all tests
swift test

# Run with verbose output  
swift test --verbose

# Run specific test suites
swift test --filter "OpenAIModelTests"
swift test --filter "StreamingTests"
swift test --filter "ToolCallingTests"

# Run tests with coverage
swift test --enable-code-coverage
```

### Test Categories

#### Unit Tests
```swift
import Testing
import Tachikoma

@Test("Message type serialization")
func messageTypeSerialization() throws {
    let message = Message.user(content: .text("Hello, world!"))
    
    let encoded = try JSONEncoder().encode(message)
    let decoded = try JSONDecoder().decode(Message.self, from: encoded)
    
    #expect(decoded == message)
}

@Test("Provider name resolution")
func providerNameResolution() async throws {
    let provider = ModelProvider.shared
    
    // Test lenient matching
    #expect(try await provider.resolveModelName("gpt") == "gpt-4.1")
    #expect(try await provider.resolveModelName("claude") == "claude-opus-4")
    #expect(try await provider.resolveModelName("grok") == "grok-4")
}
```

#### Integration Tests
```swift
@Test("OpenAI streaming response")
func openAIStreamingResponse() async throws {
    // Skip if no API key
    guard let apiKey = ProcessInfo.processInfo.environment["OPENAI_API_KEY"],
          !apiKey.isEmpty else {
        throw XCTSkip("OpenAI API key not provided")
    }
    
    let model = try await tachikoma.getModel("gpt-4.1")
    let request = ModelRequest(
        messages: [.user(content: .text("Count from 1 to 5"))]
    )
    
    var events: [StreamEvent] = []
    for try await event in try await model.getStreamedResponse(request: request) {
        events.append(event)
    }
    
    #expect(events.contains { if case .responseStarted = $0 { return true }; return false })
    #expect(events.contains { if case .textDelta = $0 { return true }; return false })
    #expect(events.contains { if case .responseCompleted = $0 { return true }; return false })
}
```

#### Performance Tests
```swift
@Test("Concurrent request handling")
func concurrentRequestHandling() async throws {
    let requestCount = 50
    let model = try await tachikoma.getModel("claude-sonnet-4")
    
    let startTime = Date()
    
    try await withThrowingTaskGroup(of: Void.self) { group in
        for i in 0..<requestCount {
            group.addTask {
                let request = ModelRequest(
                    messages: [.user(content: .text("Request \(i)"))]
                )
                _ = try await model.getResponse(request: request)
            }
        }
    }
    
    let duration = Date().timeIntervalSince(startTime)
    let requestsPerSecond = Double(requestCount) / duration
    
    print("ğŸ“Š Processed \(requestCount) requests in \(duration)s (\(requestsPerSecond) req/s)")
    #expect(requestsPerSecond > 5.0) // Should handle at least 5 requests per second
}
```

### Mock Testing Support

```swift
// Built-in mock provider for testing
class MockModelProvider: ModelInterface {
    var maskedApiKey: String = "mock-***"
    var responses: [ModelResponse] = []
    var streamEvents: [StreamEvent] = []
    
    func getResponse(request: ModelRequest) async throws -> ModelResponse {
        guard !responses.isEmpty else {
            throw TachikomaError.mockError("No mock responses configured")
        }
        return responses.removeFirst()
    }
    
    func getStreamedResponse(request: ModelRequest) async throws -> AsyncThrowingStream<StreamEvent, Error> {
        return AsyncThrowingStream { continuation in
            Task {
                for event in streamEvents {
                    continuation.yield(event)
                    try await Task.sleep(for: .milliseconds(10))
                }
                continuation.finish()
            }
        }
    }
}

// Usage in tests
@Test("Tool calling workflow")
func toolCallingWorkflow() async throws {
    let mockProvider = MockModelProvider()
    mockProvider.responses = [
        ModelResponse(
            id: "test-123",
            model: "mock-model",
            content: [
                .toolCall(ToolCallItem(
                    id: "call-123",
                    type: .function,
                    function: FunctionCall(
                        name: "get_weather",
                        arguments: "{\"location\": \"San Francisco\"}"
                    )
                ))
            ]
        )
    ]
    
    await Tachikoma.shared.registerModel(name: "mock") { mockProvider }
    
    let model = try await tachikoma.getModel("mock")
    let request = ModelRequest(
        messages: [.user(content: .text("What's the weather?"))],
        tools: [weatherToolDefinition]
    )
    
    let response = try await model.getResponse(request: request)
    
    #expect(response.content.count == 1)
    if case .toolCall(let toolCall) = response.content.first {
        #expect(toolCall.function.name == "get_weather")
    }
}
```

## ğŸ”„ Migration Guide

### From PeekabooCore AI System

If you're migrating from PeekabooCore's built-in AI functionality:

#### 1. Import Changes
```swift
// Old
import PeekabooCore

// New  
import Tachikoma
```

#### 2. Error Type Updates
```swift
// Old
catch let error as PeekabooError {
    switch error {
    case .aiProviderError(let message):
        print("AI Error: \(message)")
    }
}

// New
catch let error as TachikomaError {
    switch error {
    case .modelNotFound(let model):
        print("Model not found: \(model)")
    case .authenticationFailed:
        print("Authentication failed")
    case .rateLimited:
        print("Rate limited - retry after \(error.retryAfter ?? 60)s")
    }
}
```

#### 3. Model Instantiation
```swift
// Old
let aiService = PeekabooAIService()
let model = try await aiService.createModel(provider: .openAI, modelName: "gpt-4")

// New
let tachikoma = Tachikoma.shared
let model = try await tachikoma.getModel("gpt-4.1")
```

#### 4. Streaming Updates
```swift
// Old - PeekabooCore streaming
for try await chunk in aiService.streamResponse(request) {
    switch chunk.type {
    case .content:
        print(chunk.text)
    case .done:
        print("Complete")
    }
}

// New - Tachikoma streaming
for try await event in try await model.getStreamedResponse(request: request) {
    switch event {
    case .textDelta(let delta):
        print(delta.delta, terminator: "")
    case .responseCompleted:
        print("\nComplete")
    default:
        break
    }
}
```

### From Direct Provider SDKs

#### From OpenAI Swift SDK
```swift
// Old - Direct OpenAI
import OpenAI

let openAI = OpenAI(apiToken: "sk-...")
let query = ChatQuery(
    model: .gpt4_o,
    messages: [.user(.init(content: "Hello"))]
)
let result = try await openAI.chats(query: query)

// New - Tachikoma
import Tachikoma

let model = try await Tachikoma.shared.getModel("gpt-4o")
let request = ModelRequest(
    messages: [.user(content: .text("Hello"))]
)
let response = try await model.getResponse(request: request)
```

#### From Anthropic SDK
```swift
// Old - Direct Anthropic
import AnthropicSwiftSDK

let client = Anthropic(apiKey: "sk-ant-...")
let message = try await client.messages.create(
    model: .claude_3_5_Sonnet,
    maxTokens: 1024,
    messages: [.user("Hello")]
)

// New - Tachikoma  
let model = try await Tachikoma.shared.getModel("claude-3-5-sonnet")
let request = ModelRequest(
    messages: [.user(content: .text("Hello"))],
    settings: ModelSettings(maxTokens: 1024)
)
let response = try await model.getResponse(request: request)
```

### Configuration Migration

```swift
// Create a migration helper
struct TachikomigraMigration {
    static func migrateFromPeekabooConfig() async throws {
        // Read old PeekabooCore configuration
        let oldConfigPath = FileManager.default.homeDirectoryForCurrentUser
            .appendingPathComponent(".peekaboo/config.json")
        
        if FileManager.default.fileExists(atPath: oldConfigPath.path) {
            let configData = try Data(contentsOf: oldConfigPath)
            let oldConfig = try JSONDecoder().decode(PeekabooConfig.self, from: configData)
            
            // Convert to Tachikoma configuration
            if let openAIKey = oldConfig.openai?.apiKey {
                let config = ProviderConfiguration.openAI(apiKey: openAIKey)
                try await Tachikoma.shared.configureProvider(config)
            }
            
            if let anthropicKey = oldConfig.anthropic?.apiKey {
                let config = ProviderConfiguration.anthropic(apiKey: anthropicKey)
                try await Tachikoma.shared.configureProvider(config)
            }
            
            print("âœ… Successfully migrated PeekabooCore configuration")
        }
    }
}
```

## ğŸ”§ Troubleshooting

### Common Issues & Solutions

#### Authentication Problems

**Issue**: `TachikomaError.authenticationFailed`
```swift
// Diagnosis
func diagnoseAuthenticationIssue() async {
    do {
        let model = try await tachikoma.getModel("gpt-4.1")
        print("âœ… Model created successfully")
        print("ğŸ”‘ Masked API Key: \(model.maskedApiKey)")
        
        // Test with minimal request
        let testRequest = ModelRequest(
            messages: [.user(content: .text("Hi"))]
        )
        _ = try await model.getResponse(request: testRequest)
        print("âœ… Authentication successful")
        
    } catch TachikomaError.authenticationFailed {
        print("âŒ Authentication failed")
        print("ğŸ’¡ Solutions:")
        print("   1. Check API key is correct and not expired")
        print("   2. Verify sufficient quota/credits")
        print("   3. Check API key permissions")
        print("   4. Try regenerating the API key")
        
    } catch TachikomaError.modelNotFound(let model) {
        print("âŒ Model '\(model)' not found")
        print("ğŸ’¡ Available models:")
        await printAvailableModels()
    }
}
```

**Solutions**:
- Verify API key format: OpenAI (`sk-...`), Anthropic (`sk-ant-...`), Grok (`xai-...`)
- Check environment variables are properly set
- Ensure API key has sufficient permissions and quota
- Try regenerating the API key if it's old

#### Rate Limiting

**Issue**: `TachikomaError.rateLimited`
```swift
func handleRateLimiting() async {
    let maxRetries = 3
    var retryCount = 0
    
    while retryCount < maxRetries {
        do {
            let model = try await tachikoma.getModel("gpt-4.1")
            let request = ModelRequest(
                messages: [.user(content: .text("Your query here"))]
            )
            
            let response = try await model.getResponse(request: request)
            print("âœ… Success: \(response)")
            return
            
        } catch TachikomaError.rateLimited {
            retryCount += 1
            let delay = Double(2 << retryCount) // Exponential backoff: 2s, 4s, 8s
            print("â±ï¸ Rate limited, retrying in \(delay) seconds... (\(retryCount)/\(maxRetries))")
            try await Task.sleep(for: .seconds(delay))
            
        } catch {
            print("âŒ Unrecoverable error: \(error)")
            break
        }
    }
    
    print("âŒ Failed after \(maxRetries) retries")
}
```

#### Model Not Found

**Issue**: `TachikomaError.modelNotFound`
```swift
func debugModelResolution() async {
    print("ğŸ” Available Models:")
    
    let commonModels = [
        "gpt-4.1", "gpt-4o", "o3", "o4-mini",
        "claude-opus-4", "claude-sonnet-4", "claude-3-5-sonnet",
        "grok-4", "grok-3", "grok-2",
        "llama3.3", "llava:latest"
    ]
    
    for modelName in commonModels {
        do {
            let model = try await tachikoma.getModel(modelName)
            print("âœ… \(modelName) - Available (\(model.maskedApiKey))")
        } catch {
            print("âŒ \(modelName) - Not available (\(error.localizedDescription))")
        }
    }
    
    print("\nğŸ’¡ Lenient matching examples:")
    print("   'gpt' â†’ 'gpt-4.1'")
    print("   'claude' â†’ 'claude-opus-4'")
    print("   'grok' â†’ 'grok-4'")
    print("   'llama' â†’ 'llama3.3'")
}
```

#### Network Issues

**Issue**: `TachikomaError.networkError`
```swift
func diagnoseNetworkIssue(_ error: TachikomaError) {
    if case .networkError(let underlying) = error {
        print("ğŸŒ Network Error Details:")
        print("   Error: \(underlying.localizedDescription)")
        
        if let urlError = underlying as? URLError {
            switch urlError.code {
            case .timedOut:
                print("   â° Timeout - try increasing timeout or check connection")
            case .notConnectedToInternet:
                print("   ğŸ“µ No internet connection")
            case .cannotConnectToHost:
                print("   ğŸ”— Cannot connect to server - check URL and firewall")
            case .cancelled:
                print("   â¹ï¸ Request was cancelled")
            default:
                print("   ğŸ”§ URL Error Code: \(urlError.code.rawValue)")
            }
        }
        
        print("\nğŸ’¡ Solutions:")
        print("   1. Check internet connection")
        print("   2. Verify firewall/proxy settings")
        print("   3. Try different network")
        print("   4. Check provider status pages")
    }
}
```

#### Memory Issues

**Issue**: High memory usage during streaming
```swift
func optimizeMemoryUsage() async throws {
    print("ğŸ§  Memory Usage Optimization Tips:")
    
    // 1. Use streaming for large responses
    let model = try await tachikoma.getModel("claude-opus-4")
    let request = ModelRequest(
        messages: [.user(content: .text("Write a very long story"))]
    )
    
    // âœ… Good - constant memory usage
    var wordCount = 0
    for try await event in try await model.getStreamedResponse(request: request) {
        switch event {
        case .textDelta(let delta):
            wordCount += delta.delta.split(separator: " ").count
            // Process immediately, don't accumulate
            
        case .responseCompleted:
            print("ğŸ“Š Processed \(wordCount) words with minimal memory impact")
        default:
            break
        }
    }
    
    // 2. Clear model cache if needed
    await ModelProvider.shared.clearCache()
    print("ğŸ—‘ï¸ Model cache cleared")
    
    // 3. Use appropriate context lengths
    let shortRequest = ModelRequest(
        messages: [.user(content: .text("Short query"))],
        settings: ModelSettings(maxTokens: 100) // Limit response size
    )
}
```

### Debugging Tools

```swift
// Enable debug logging
extension Tachikoma {
    func enableDebugLogging() {
        // Add debug configuration
        UserDefaults.standard.set(true, forKey: "TachikomaDebugLogging")
    }
    
    func printDiagnostics() async {
        print("ğŸ” Tachikoma Diagnostics")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        
        // Environment variables
        print("ğŸŒ Environment:")
        let envVars = ["OPENAI_API_KEY", "ANTHROPIC_API_KEY", "X_AI_API_KEY", "XAI_API_KEY"]
        for envVar in envVars {
            let value = ProcessInfo.processInfo.environment[envVar]
            if let value = value, !value.isEmpty {
                let masked = String(value.prefix(6)) + "..." + String(value.suffix(2))
                print("   \(envVar): \(masked)")
            } else {
                print("   \(envVar): âŒ Not set")
            }
        }
        
        // Model provider status
        print("\nğŸ¤– Model Status:")
        let testModels = ["gpt-4.1", "claude-opus-4", "grok-4", "llama3.3"]
        for modelName in testModels {
            do {
                let model = try await getModel(modelName)
                print("   âœ… \(modelName): \(model.maskedApiKey)")
            } catch {
                print("   âŒ \(modelName): \(error.localizedDescription)")
            }
        }
        
        // System info
        print("\nğŸ’» System:")
        print("   Platform: \(ProcessInfo.processInfo.operatingSystemVersionString)")
        print("   Memory: \(ProcessInfo.processInfo.physicalMemory / 1024 / 1024 / 1024)GB")
        print("   Processors: \(ProcessInfo.processInfo.processorCount)")
    }
}

// Usage
await Tachikoma.shared.printDiagnostics()
```

### Provider-Specific Issues

#### OpenAI Issues
- **O3/O4 Models**: Use Responses API automatically, don't set temperature
- **Rate Limits**: Tier-based, check usage dashboard
- **Context Length**: Varies by model (4K-1M tokens)

#### Anthropic Issues  
- **System Prompts**: Must be separate from conversation messages
- **Content Blocks**: Images must be base64 encoded
- **Rate Limits**: Per-minute and per-day limits

#### Grok Issues
- **Parameter Filtering**: Grok 4 doesn't support frequency/presence penalties
- **API Keys**: Support both `X_AI_API_KEY` and `XAI_API_KEY`
- **Rate Limits**: Check xAI documentation for current limits

#### Ollama Issues
- **Local Connection**: Ensure Ollama is running (`ollama serve`)
- **Model Loading**: First request may take 30-60 seconds
- **Tool Calling**: Only supported on specific models
- **Memory**: Large models require significant RAM

## ğŸ¤ Contributing

We welcome contributions to Tachikoma! Here's how to get involved:

### Development Setup

```bash
# Clone the repository
git clone https://github.com/steipete/Tachikoma.git
cd Tachikoma

# Verify Swift 6 installation
swift --version  # Should be 6.0+

# Run tests to ensure everything works
swift test

# Build the package
swift build
```

### Code Style Guidelines

- **Swift 6 Strict Mode**: All code must compile with strict concurrency
- **Type Safety**: Prefer strong typing over `Any` or type erasure
- **Actor Safety**: Use `@MainActor` and `Sendable` appropriately
- **Documentation**: All public APIs must have documentation comments
- **Testing**: New features require comprehensive test coverage

### Contribution Process

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Write** your code following our style guidelines
4. **Add** comprehensive tests for your changes
5. **Update** documentation if needed
6. **Ensure** all tests pass (`swift test`)
7. **Commit** your changes (`git commit -m 'Add amazing feature'`)
8. **Push** to your branch (`git push origin feature/amazing-feature`)
9. **Open** a Pull Request

### Areas for Contribution

- ğŸ”Œ **New Providers**: Add support for additional AI services
- ğŸ§ª **Testing**: Improve test coverage and add integration tests
- ğŸ“š **Documentation**: Enhance guides, examples, and API docs
- âš¡ **Performance**: Optimize networking, caching, and memory usage
- ğŸ”§ **Tooling**: Development tools, debugging utilities, linting
- ğŸ› **Bug Fixes**: Report and fix issues

### Provider Implementation Guide

To add a new AI provider:

1. **Create Provider Directory**: `Sources/Providers/YourProvider/`
2. **Implement ModelInterface**: Core protocol conformance
3. **Add Provider Types**: Request/response types in `YourProviderTypes.swift`
4. **Register Provider**: Add to default registrations
5. **Write Tests**: Comprehensive test coverage
6. **Update Documentation**: Add to README and architecture docs

```swift
// Example provider implementation structure
class YourProviderModel: ModelInterface {
    var maskedApiKey: String { /* implementation */ }
    
    func getResponse(request: ModelRequest) async throws -> ModelResponse {
        // Implementation
    }
    
    func getStreamedResponse(request: ModelRequest) async throws -> AsyncThrowingStream<StreamEvent, Error> {
        // Implementation  
    }
}
```

## ğŸ“„ License

Tachikoma is available under the MIT License. See [LICENSE](LICENSE) for details.

## ğŸ’¬ Support

- **ğŸ› Bug Reports**: [GitHub Issues](https://github.com/steipete/Tachikoma/issues)
- **ğŸ’­ Feature Requests**: [GitHub Discussions](https://github.com/steipete/Tachikoma/discussions)
- **ğŸ“– Documentation**: [API Reference](https://steipete.github.io/Tachikoma/)
- **ğŸ†˜ Help & Questions**: [GitHub Discussions Q&A](https://github.com/steipete/Tachikoma/discussions/categories/q-a)

## ğŸ™ Acknowledgments

- Named after the **Tachikoma** spider-tank AI from Ghost in the Shell
- Inspired by the Swift community's commitment to type safety and performance
- Built on the foundations of modern Swift concurrency and actor-based programming
- Special thanks to all contributors and the AI provider communities

---

<div align="center">
  <strong>Built with â¤ï¸ for the Swift AI community</strong>
  <br>
  <sub>ğŸ•·ï¸ Intelligent â€¢ Adaptable â€¢ Reliable</sub>
</div>